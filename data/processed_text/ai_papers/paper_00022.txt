\title{
Phased Training for Improved Motion ImitATION: A Study in Reward Balancing for AMP
}

\begin{abstract}
Physics-based character animation requires balancing task completion with motion style preservation, a challenge that becomes particularly acute in deep reinforcement learning approaches like AMP (Peng et al., 2021). While existing methods use fixed reward weights, we observe that the relative importance of these objectives varies significantly across motion types and training phases. We address this through a phased training approach that introduces controlled adaptation periods and bounded weight adjustments, focusing on stability during early learning. Our experiments with walking, jogging, and running motions demonstrate significant improvements over the AMP baseline, achieving 32% higher discriminator rewards for walking (1.02 to 1.35) and 17 % for jogging (1.01 to 1.18), while maintaining performance for running motions. These improvements emerge primarily from our phased training strategy, which proves effective even when direct weight adaptation faces architectural constraints, suggesting broader applications for stabilizing multi-objective learning in character animation.
\end{abstract}
\section*{1 INTRODUCTION}
Physics-based character animation through deep reinforcement learning has become essential for creating realistic virtual characters, with methods like DeepMinic [Peng et al., and AMP [Peng et al., showing promising results. However, these approaches face a fundamental challenge: balancing task completion with motion style preservation. Motion style preservation, a longstanding challenge in character animation (Crochow et al.; 2004), becomes particularly complex when combined with physical constraints and task objectives.
Our baseline experiments reveal the difficulty of this balance, with discriminator rewards varying significantly across motion types (1.02 for walking, 1.01 for jogging, 0.54 for running). Traditional approaches using static reward weights struggle with these variations (Qiu et al., 2024) [Wang & [Beltrame , terekhov & Gulcehre, 2024, leading to a critical trade-off: strict adherence to reference motions can impede task completion, while aggressive task optimization often produces unnatural movements. This challenge is particularly acute in multi-objective reinforcement learning. The finding optimal policies requires careful consideration of competing objectives (Qiu et al. 2024).
We address this challenge through a phased training approach that carefully manages the transition between objectives. Rather than attempting direct weight adaptation, which proved challenging due to architectural constraints, we introduce controlled adaptation periods that allow the policy to develop stable behaviors before fine-tuning. Our method uses exponential moving averages (\alpha=0.9) to track reward magnitudes and implements bounded adjustments (0.2-5.0), with adaptation beginning after 20 % of training completion.
Our primary contributions include:
- A phased training strategy that improves motion quality without requiring architectural modifications to the base AMP framework
- An empirical study of reward balancing dynamics in motion imitation, revealing the importance of training phase management
- Comprehensive evaluation showing significant improvements in discriminator rewards for walking (32% gain, 1.02 to 1.35 ) and jogging (17% gain, 1.01 to 1.18 ) while maintaining running performance
Through extensive experiments, we demonstrate that our phased approach consistently improves or maintains motion quality across all tested scenarios. While the improvements vary by motion type, with walking and jogging showing the most significant gains, our method's success without direct weight adaptation suggests broader applications for stabilizing multi-objective learning in character animation. These findings point to the importance of carefully managed training progression over complex architectural modifications.
\section*{2 RELATED WORK}
Our work builds on several research threads in physics-based character animation and multi-objective reinforcement learning. Early approaches like SIMBICON (Jin et al., 2007) demonstrated the effectiveness of simple feedback control, but lacked the adaptability needed for complex motions. In contrast, our method leverages deep learning while maintaining stable control through careful training phases.
Motion imitation approaches have evolved from hand-crafted objectives to learned representations. DeepMimic [Peng et al., pioneered the use of reference motion tracking but required extensive reward engineering. While VAE-based methods (Ling et al., 2020) learned motion representations automatically, they struggled with physical constraints that our approach explicitly maintains through phased training. Model predictive control methods [Eom et al., achieve high precision but require significant computational resources compared to our real-time capable approach.
The challenge of balancing multiple objectives has been addressed through various strategies. DReCon (Bergamin et al., 2019) introduced data-driven reward design but maintained fixed weights, while [Coros et al., used structured but static objective weighting. Our dynamic adaptation strategy differs by automatically adjusting weights based on training progress and reward magnitudes. Recent work in dialogue systems [Ultes et al., demonstrated the benefits of adaptive reward balancing, though their methods required modifications to work with the physical constraints in the model. [Jin et al., 2019, 2019]
AMP [Peng et al., revolutionized style preservation by replacing hand-crafted rewards with learned discriminators, but used fixed reward weights throughout training. Extensions like ASE [Peng et al., and CALM [fessler et al., 2023] improved motion quality through skill embeddings and enhanced control but maintained static relationships between objectives. Our approach complements these advances by introducing dynamic weight adaptation while preserving their core benefits. Unlike previous attempts at multi-objective balancing in reinforcement learning [Petz & Hotegni , we specifically address the unique challenges of physical motion synthesis through our phased training strategy.
\section*{3 BACKGROUND}
Motion imitation in physics-based character animation has evolved from trajectory optimization (AI Borno et al., 2013) to deep learning approaches that balance physical realism with style preservation. DeepMimic [Peng et al., demonstrated the effectiveness of combining reference motion tracking with physical simulation, though it required extensive reward engineering. AMP [Peng et al., 2021) advanced this by replacing hand-crafted rewards with learned discriminators (Ho & Ermon, 2016), but introduced new challenges in balancing multiple objectives [Peitz & [Hotegni .
\subsection*{3.1 Problem Setting}
We formulate motion imitation as a reinforcement learning problem with state space \mathcal{S} and action space \mathcal{A}. At time t, the state s_t \in \mathcal{S} comprises joint positions and velocities, while actions \alpha_t \in \mathcal{A} represent joint torques. The goal is to learn a policy \pi_\theta(\alpha_t \mid s_t) that optimizes a weighted combination
of task completion and motion style preservation:
\[
r_{\text {total }}(s_t, a_t)=w_{\text {task }} r_{\text {task }}(s_t, a_t)+w_{\text {Style }} r_{\text {style }}(s_t, a_t)
\]
where r_{\text {task }} measures goal achievement and r_{\text {style }} quantifies motion naturalness through a learned discriminator.
Traditional approaches use static weights w_{\text {task }} and w_{\text {Style }}, which our baseline experiments show leads to varying performance (discriminator rewards: 1.02 walking, 1.01 jogging, 0.54 running). Our method introduces dynamic weight adjustment with bounds [0.2,5. and delayed adaptation onset (20% of training), requiring:
- Reference motions for style imitation
- Separable reward components
- Stability under bounded weight adjustments
\section*{4 METHOD}
Building on the problem formulation from Section . we introduce a phased training approach that improves motion quality through careful management of reward balancing. Our method consists of three key components: reward tracking through exponential moving averages (EMAs), bounded weight adjustment, and a phased training schedule.
The reward tracking system maintains EMAs of task and style reward magnitudes:
\[
\begin{array}{l}
MA_{\text {task }}^{t}=\alpha M_{\text {task }}^{t}+(1-\alpha)\left|r_{\text {task }}(s_t, a_t)\right| 

MA_{\text {style }}^{t}=\alpha M_{\text {style }}^{t-1}+(1-\alpha)\left|r_{\text {style }}(s_t, a_t)\right|
\end{array}
\]
where \alpha=0.9 was chosen based on early experiments showing instability with more aggressive rates (\alpha=0.8). These averages provide a stable estimate of reward magnitudes while remaining responsive to changes in motion quality.
To prevent extreme weight adjustments that could destabilize training, we compute preliminary weights with strict bounds:
\[
\begin{array}{l}
\hat{w}_{\text {task }}=\operatorname{clip}(\frac{MA_{\text {style }}^{t}}{MA_{\text {task }}^{t}+MA_{\text {style }}^{t}}, 0.2,5.0) 

\hat{w}_{\text {style }}=\operatorname{clip}(\frac{MA_{\text {task }}^{t}}{MA_{\text {task }}^t+MA_{\text {style }}^{t}}, 0,2.5,0)
\end{array}
\]
followed by normalization to ensure proper scaling:
\[
w_{\text {task }}=\frac{\hat{w}_{\text {task }}}{\hat{w}_{\text {task }}+\hat{w}_{\text {style }}}, \quad w_{\text {style }}=\frac{\hat{w}_{\text {style }}}{\hat{w}_{\text {task }}+\hat{w}_{\text {Style }}}
\]
Our key insight is the importance of establishing stable baseline behavior before attempting reward adaptation. This leads to a three-phase training approach:
1. Initial Phase (0-20%): Fixed equal weights allow the policy to develop basic competence
2. Transition Phase (20-30%): Gradual introduction of computed weights through linear interpolation
3. Adaptive Phase (30-100%): Full weight adjustment updated every 100 steps
The final reward computation follows the standard AMP structure but with time-varying weights:
\[
r_{\text {total }}(s_t, a_t )=w_{\text {task }}(t) r_{\text {task }}(s_t, a_t)+ w_{\text {Style }}(t) r_{\text {style }}(s_t, a_t)
}
\]
While our attempts at direct weight modification were constrained by AMP's architecture, the phased training approach alone proved sufficient to significantly improve motion quality, particularly for walking and jogging motions.
\section*{5 EXPERIMENTAL SETUP}
We evaluate our method using the DeepMimic framework (Peng et al., 2018) with three reference motions (walking, jogging, running) from the standard DeepMimic motion capture dataset. All experiments use a 42-degree-of-freedom humanoid character simulated in the Bullet physics engine at 60 FPS, with 10 update substeps and 2 simulation substeps per frame for numerical stability.
Our implementation extends AMP through four experimental iterations, each refining the weight adaptation approach:
- Run 1: Basic EMA tracking (\alpha=0.95)
- Run 2: Aggressive adaptation (\alpha=0.8) with bounded weights [0.2,5.
- Run 3: Moderate adaptation (\alpha=0.9) with phased training
- Run 4: Direct reward system integration attempts
The final implementation uses PPO (Schulman et al. 2017) with network architecture and hyperparameters tuned for stability:
- Policy/Value/Discriminator networks: 2 layers, 1024 units each
- Learning rates: 2 \times 10^-4 (actor), 1 \times 10^-3 (critic)
- PPO clip ratio: 0.2 , discount factor: 0.95
- Buffer size: 100,000 transitions, batch size: 32
- Weight updates: Every 100 steps
We evaluate performance through three complementary metrics:
- Discriminator reward: Primary metric for motion naturalness
- Pose error: Weighted combination of position (10%), rotation (20%), and joint angles (70%)
- Task completion: Target reaching while maintaining balance
Each experiment runs for 10,000 steps with metrics collected every 100 steps. We conduct multiple runs per motion type to account for training variability and focus particularly on discriminator rewards as our primary quality metric.
\section*{6 RESULTS}
We conducted a systematic evaluation through four experimental iterations, each building on insights from the previous run. Our baseline AMP implementation established reference performance levels of 1.02 (walking), 1.01 (jogging), and 0.54 (running) for discriminator rewards, revealing inherent differences in motion complexity.
Our experimental progression revealed key insights about training stability and adaptation:
- Run 1 (Basic EMA, \alpha=0.95 ): Initial attempt showed mixed results (walking: 0.90, jogging: 0.54, running: 1.39), with unstable performance suggesting the need for more controlled adaptation
- Run 2 (Aggressive EMA, \alpha=0.8 ): Faster adaptation improved jogging (1.17) but destabilized running (0.45), highlighting sensitivity to adaptation rates
- Run 3 (Phased Training, \alpha=0.9 ): Achieved best results through delayed adaptation:
- Walking: 32 % improvement (1.02 to 1.35 )
- Jogging: 17 % improvement (1.01 to 1.18 )
- Running: Maintained baseline ( 0.54 to 0.55 )
- Run 4 (Direct Integration): Attempted reward system modification but maintained Run 3's performance levels, revealing architectural limitations
Discriminator Loss During Training

Figure 1: Training progression and final performance analysis. Left: Discriminator loss showing initial instability before 20 % completion, followed by convergence during transition (20-30%). Middle: Final discriminator rewards demonstrating significant improvements for walking (1.35) and jogging (1.18). Right: Weighted pose errors (position 10%, rotation 20%, joint angles 70%) showing consistent performance maintenance.
As shown in Figure 1 the phased training approach significantly improved stability and performance, particularly during the critical 20-30 % transition period. However, our attempts to implement dynamic weight adaptation faced consistent challenges:
- Architectural Constraints: All runs maintained final weights at 0.5 despite various integration attempts, suggesting deeper structural limitations
- Motion Complexity: Effectiveness correlated with baseline performance (32% improvement for walking vs. 2 % for running)
- Parameter Sensitivity: Performance varied significantly between adaptation rates (\alpha=0.8 vs. \alpha=0.9)
These results demonstrate that while our phased training approach successfully improved motion quality, particularly for stable gaits, the implementation of truly dynamic reward balancing may require fundamental modifications to the AMP transchitecture. The consistent improvements in walking and jogging motions, achieved without successful weight adaptation, suggest that careful training phase management may be more crucial than dynamic reward balancing for stable motion imitation.
\section*{7 CONCLUSIONS AND FUTURE WORK}
We presented a phased training approach for motion imitation that significantly improves performance without requiring architectural modifications to the AMP framework (Peng et al., ). Our key finding is that carefully managed training phases-with a 20 % initial stabilization period and gradual transition-can achieve substantial improvements in motion quality even without successful dynamic weight adaptation. This approach improved walking motions by 32 %(1.02 to 1.35) and jogging by 17 %(1.01 to 1.18), while maintaining running performance (0.54 to 0.55).
Our experimental progression revealed that while direct weight adaptation proved challenging due to AMP's architecture (evidenced by consistent 0.5 weights across all runs), the phased training approach alone provided significant benefits. Early experiments with aggressive adaptation (\alpha=0.8) highlighted the importance of stability, showing degraded running performance (0.45) compared to baseline (0.54). The final approach with \alpha=0.9 and phased training achieved the best balance of stability and improvement.
These findings suggest several promising research directions:
- A modular reward architecture that separates weight management from core network components, potentially enabling true dynamic adaptation
Mikhail Terekhov and Caglar Gulcehre. In search for architectures and loss functions in multiobjective reinforcement learning. ArXiv, abs/2407.16807, 2024.
Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, and Xue Bin Peng. CALM: Conditional adversarial latent models for directable virtual characters. ACM Transactions on Graphics, 2023. doi: 10.1145/3592440. URL https://doi.org/10.1145/3592440
Chen Tessler, Yunrong Guo, Ofir Nabati, Gal Chechik, and Xue Bin Peng. MaskedMimic: Unified physics-based character control through masked motion inpainting. ACM Transactions on Graphics, 43(6), December 2024. ISSN 0730-0301. doi: 10.1145/3687951. URL https://doi.org/ 10.1145/3687951
Gefan Ultes, Pawel Budzianowski, I. Casanueva, N. Mrksic, L. Rojas-Barahona, Pei hao Su, TsungHsen Wen, M. Gášić, and S. Young. Reward-balancing for statistical spoken dialogue systems using multi-objective reinforcement learning. pp. 65-70, 2017.
Dong Wang and Giovanni Beltrame. Moseac: Streamlined variable time step reinforcement learning. ArXiv, abs/2406.01521, 2024.
KangKang Yin, K. Loken, and M. V. D. Panne. SIMBICON: simple biped locomotion control. 2007.